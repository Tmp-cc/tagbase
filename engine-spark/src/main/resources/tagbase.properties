# engine-spark
tagbase.job.type=spark
tagbase.job.spark.default.driverMemory=800m
tagbase.job.spark.default.executorMemory=500m
tagbase.job.spark.default.executorInstances=1
tagbase.job.spark.default.memoryOverhead=1000
tagbase.job.spark.default.queue=root.default
tagbase.job.spark.default.executorCores=1
tagbase.job.spark.default.jarPath=D:\\workStation\\job-spark-1.0-jar-with-dependencies.jar
tagbase.job.spark.default.master=local[4]
tagbase.job.spark.default.deployMode=client
tagbase.job.spark.default.trackUrl=http://bjht1258:8088/ws/v1/cluster/apps/
tagbase.job.spark.default.user=80265599
tagbase.job.spark.default.shardItems=80000
tagbase.job.spark.default.parallelism=3
tagbase.job.spark.default.sparkLogPath=D:\\workStation\\tagbase\\log
tagbase.job.spark.default.logVerbose=true
tagbase.job.spark.default.waitAppCompletion=false
tagbase.job.spark.default.maxAppAttempts=2
tagbase.job.spark.default.eventIdColumnName=galileo_event_id


#just for example test
tagbase.job.dict.inverted.base.path=D:/workStation/tagbase/invertedDict
tagbase.job.work.base.dir=D:/workStation/tagbase/bitmapData
tagbase.job.dict.input.hive.db=default
tagbase.job.dict.input.hive.table=imeiTable
tagbase.job.dict.input.hive.column=imei
tagbase.job.dict.input.hive.column.type=STRING
tagbase.job.dict.input.hive.part_column=dayno
tagbase.job.dict.input.hive.part_column.type=NUMBER
tagbase.job.dict.input.hive.part_column.format=HIVE_DATE

